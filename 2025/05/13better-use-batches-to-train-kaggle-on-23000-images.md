---
title: "better use batches to train Kaggle on 23000 images"
tags: [ "2025", "haha", "kaggle", "pop" ]
author: Rob Nugen
date: 2025-05-13T22:44:00+09:00
draft: false
---

Can you write a poem about the situation in this screenshot?

[![2025 may 13 train kernel unlike a boss 1000](//b.robnugen.com/journal/2025/thumbs/2025_may_13_train_kernel_unlike_a_boss.png)](//b.robnugen.com/journal/2025/2025_may_13_train_kernel_unlike_a_boss_1000.png)

::: ai chatgpt

Absolutely â€” hereâ€™s a poem for this dramatic moment:

---

ğŸ’” **Ode to the Fallen Kernel**

**Twas hour one, a noble quest,**
To train a model with our best.
Images lined in tidy rows,
URLs and labels â€” off it goes.

**Each byte was fetched with earnest pride,**
From CloudFront's vaults both far and wide.
Twenty-three thousand, queued and true,
Untilâ€¦ the RAM it *overgrew*.

**A quiet snap â€” a puff, a sigh,**
The memory bar had climbed too high.
Kaggleâ€™s whisper: *â€œYou asked too much,â€*
And shut it down with just one touch.

**A box appeared, so cold and flat,**
â€œKernel Restarting.â€ Just like that.
No tear, no beep, no drawn-out fade â€”
Just dreams reset and work unsaved.

But we, undaunted, shall not yield.
We'll batch, weâ€™ll split, weâ€™ll stay the field.
For files live in code and flame,
And models rise â€” despite the blame.

:::